{"cells":[{"cell_type":"markdown","source":["#Emotion Recognition in Video\n","A Deep Learning Project by: <br>\n"Sahar Machavy <br>\n","Omri Shmueli <br>\n","Nir Dar"],"metadata":{"id":"sZmdZWx-WudI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCEdUPvX-GsA"},"outputs":[],"source":["!pip install deepface\n","!pip install opencv-python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaTJ0oMG-Lvi"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import math\n","import matplotlib.pyplot as plt\n","from matplotlib.image import imsave\n","from matplotlib.animation import FuncAnimation\n","import matplotlib.animation as animation\n","import cv2\n","import IPython.display as ipd\n","from tqdm import tqdm\n","import subprocess\n","from IPython.display import Video\n","from deepface import DeepFace\n","import os\n","from skimage.util import img_as_ubyte\n","from scipy.signal import wiener, welch\n","from IPython.display import HTML\n","from base64 import b64encode\n","import itertools\n","from skimage import color, data, restoration\n","from sklearn.metrics import confusion_matrix, accuracy_score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLgiZWSS-Qfp"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"0T23cEzAsdnR"},"source":["Define working directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Uwm6jQJsiCg"},"outputs":[],"source":["base_path = \"/content/drive/My Drive/Colab Notebooks/Final Project/\" # Change me if needed\n","if not os.path.exists(base_path + \"outputs/\"):\n","          os.mkdir(base_path + \"outputs/\")"]},{"cell_type":"markdown","metadata":{"id":"TpKQxW2ZJi28"},"source":["# Class for working with labeled videos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUKboZwfJjLo"},"outputs":[],"source":["class LabeledVideo:\n","    \n","    def __init__(self, video_name, video_path, labels_path):\n","        self.video_name = video_name\n","        self.video_path = video_path\n","        self.frames_directory = None\n","        self.frames_wip_directory = None\n","        self.labels_path = labels_path\n","        df = pd.read_csv(self.labels_path)\n","        self.df = df \n","        self.labels = df.iloc[:,1:]\n","        self.sorted_frames = None\n","        self.predictions_simple = []\n","        self.prob_predictions_simple = []\n","        self.accuracy_simple = None\n","        self.predictions = []\n","        self.prob_predictions = []\n","        self.accuracy = None\n","        self.boxes = []\n","        self.predicted_video_path = None\n","\n","        \n","    def save_video_frames(self, fixed_frames_per_sec):\n","        video_file = self.video_path\n","        video_name = self.video_name\n","        frame_num = 0\n","        cap = cv2.VideoCapture(video_file)\n","        fps = int(cap.get(cv2.CAP_PROP_FPS))\n","        frames_cut = math.floor(fps / fixed_frames_per_sec)\n","        frame_count = 0\n","        \n","        if not os.path.exists(base_path + f\"frames/{video_name}\"):\n","          os.mkdir(base_path + f\"frames/{video_name}\")\n","        if not os.path.exists(base_path + f\"frames/{video_name}/labeled_frames\"):\n","          os.mkdir(base_path + f\"frames/{video_name}/labeled_frames\")\n","        if not os.path.exists(base_path + f\"frames/{video_name}/wip_frames\"):\n","          os.mkdir(base_path + f\"frames/{video_name}/wip_frames\")\n","        self.frames_directory = base_path + f\"frames/{video_name}/labeled_frames/\"\n","        self.frames_wip_directory = base_path + f\"frames/{video_name}/wip_frames/\"\n","        \n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame_count += 1\n","            if frame_count % frames_cut == 0:\n","              cv2.imwrite(base_path + f\"frames/{video_name}/labeled_frames/frame_{frame_num}.png\", frame)\n","              cv2.imwrite(base_path + f\"frames/{video_name}/wip_frames/frame_{frame_num}.png\", frame)\n","              frame_num +=1\n","        cap.release()\n","        files = os.listdir(self.frames_directory)\n","        files = sorted(files, key=lambda x: int(\"\".join([i for i in x if i.isdigit()])))\n","        self.sorted_frames = files\n","\n","\n","    def calculate_simple_accuracy(self):\n","      \n","      if self.predictions_simple == []: # No predictions to calculate for\n","        return\n","      elif self.accuracy_simple == None: # Once calculated no need to calculate more, won't change\n","        acc_simple = accuracy_score(self.labels, self.predictions_simple)\n","        self.accuracy_simple = acc_simple\n","\n","    def calculate_accuracy(self):\n","\n","      if self.predictions == []: # No predictions to calculate for\n","        return\n","      acc = accuracy_score(self.labels, self.predictions)\n","      self.accuracy = acc  \n","    \n","    def plot_confusion_matrix(self, prediction_type):\n","      if prediction_type == 'simple': \n","        pred = self.predictions_simple\n","      elif prediction_type == 'full':\n","        pred = self.predictions\n","      if pred == []:\n","        return \"No predictions were made\"\n","      labels = ['neutral','sad','angry','disgust','happy','surprise', 'fear']\n","      cm = confusion_matrix(self.labels, pd.DataFrame(pred), labels = labels)\n","      accuracy = np.trace(cm) / np.sum(cm).astype('float')\n","      misclass = 1 - accuracy\n","      cmap = plt.get_cmap('Blues')\n","      normalize=False\n","      target_names = labels\n","      plt.figure(figsize=(8, 6))\n","      plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","      plt.title(self.video_name+\" Confusion Matrix\")\n","      plt.colorbar()\n","\n","      if target_names is not None:\n","          tick_marks = np.arange(len(target_names))\n","          plt.xticks(tick_marks, target_names, rotation=45)\n","          plt.yticks(tick_marks, target_names)\n","\n","      if normalize:\n","          cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","\n","      thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","      for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","          if normalize:\n","              plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                          horizontalalignment=\"center\",\n","                          color=\"white\" if cm[i, j] > thresh else \"black\")\n","          else:\n","              plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                          horizontalalignment=\"center\",\n","                          color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","\n","      plt.tight_layout()\n","      plt.ylabel('True label')\n","      plt.xlabel('Predicted label\\n\\n\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n","      plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6ShI5WCrzsOe"},"source":["Functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A8nkx2EgzrzP"},"outputs":[],"source":["def make_predictions(videos, chosen_detector, prediction_type):\n","  \"\"\"\n","  Expect a list of the videos object to analyze, a chosen detector backend and the type of prediction that need to be made\n","  Returns a dictionary of the emotion probs for each video and a dictionary of the dominant emotion for each video  \n","  \"\"\"\n","\n","  emotion_prob = {}\n","  dom_emotion = {}\n","\n","  for video in videos:\n","    temp_prob = []\n","    temp_emo = []\n","    for frame in video.sorted_frames:\n","      if prediction_type == \"simple\": \n","        obj = DeepFace.analyze(img_path = video.frames_directory+frame, actions = ['emotion', 'dominant_emotion'],\n","                             detector_backend= chosen_detector, enforce_detection= False, silent = True)\n","      elif prediction_type == \"full\":\n","        obj = DeepFace.analyze(img_path = video.frames_wip_directory+frame, actions = ['emotion', 'dominant_emotion'],\n","                             detector_backend= chosen_detector, enforce_detection= False, silent = True)\n","      temp_prob.append(obj[0]['emotion'])\n","      temp_emo.append(obj[0]['dominant_emotion'])\n","    emotion_prob[video.video_name] = temp_prob\n","    dom_emotion[video.video_name] = temp_emo\n","  return emotion_prob, dom_emotion\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lyRYDcdB3ffi"},"outputs":[],"source":["def variance_of_laplacian(image):\n","    gray_im = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","    # compute the Laplacian of the image and then compute the variance\n","    gray_im = cv2.GaussianBlur(gray_im, (3,3), 0) # Removes high-frequency noise from the image, which can improve the performance of the Laplacian filter later.\n","    laplacian = cv2.Laplacian(gray_im, cv2.CV_32F)\n","    return laplacian.var() "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzQmJkhd6eHw"},"outputs":[],"source":["def plot_emotion_range(video: LabeledVideo, prediction_type: str):\n","  \n","  lab = list(np.concatenate(video.labels.values))\n","  if prediction_type == 'simple':\n","    pred = list(np.array(video.predictions_simple))\n","  elif prediction_type == 'full':\n","    pred = list(np.array(video.predictions))\n","\n","  emotions = {'disgust': -4, 'sad': -3, 'angry': -2, 'fear': -1, 'neutral': 0, 'surprise': 1, 'happy': 2}\n","  lab = list(map(lambda x: emotions[x], lab))\n","  pred = list(map(lambda x: emotions[x], pred))\n","  comparison = pd.DataFrame({\"Label\": lab, \"Prediction\": pred})\n","\n","  x = np.array(range(len(pred)))\n","  plt.figure(figsize=(15,10))\n","\n","  sorted_emotions = sorted(emotions, key=emotions.get)\n","\n","  plt.ylim((min(emotions.values())-0.5, max(emotions.values())+0.5))\n","  plt.yticks(sorted(emotions.values()), sorted_emotions)\n","  plt.title(video.video_name +\" emotion range\")\n","  plt.plot(x, 'Label', data=comparison, marker='o', markerfacecolor='blue', markersize=6, color='skyblue', linewidth=4)\n","  plt.plot(x, 'Prediction', data=comparison, marker='o', markerfacecolor='black', markersize=6, color='orange', linewidth=4)\n","  plt.xlabel(\"Frame\")\n","  plt.ylabel(\"Emotion\")\n","  plt.legend()\n","  plt.grid()\n","  plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XW9md0KIaDyz"},"outputs":[],"source":["def find_misclassed_frames(video: LabeledVideo, prediction_type: str):\n","  misclassed = []\n","  if prediction_type == 'simple':\n","     pred = list(np.array(video.predictions_simple))\n","  elif prediction_type == 'full':\n","     pred = list(np.array(video.predictions))\n","\n","  labels = list(np.concatenate(video.labels.values))\n","  for i in range(len(labels)):\n","    if labels[i] != pred[i]:\n","      misclassed.append(i)\n","  return misclassed\n","\n","def show_misclassed_frames(video: LabeledVideo, prediction_type: str):\n","  misclassed_frames = find_misclassed_frames(video, prediction_type)\n","  video_labels = list(np.concatenate(video.labels.values))\n","  for i in misclassed_frames:\n","    # Load the image from file\n","    img = plt.imread(video.frames_directory+f\"frame_{i}.png\")\n","\n","    # Create a figure with two subplots\n","    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 10))\n","\n","    # Show the image in the first subplot\n","    ax1.imshow(img)\n","    ax1.axis('off')\n","    labels = []\n","    pb = []\n","    if prediction_type == 'simple':\n","      probabilities = video.prob_predictions_simple[i]\n","    elif prediction_type == 'full':\n","      probabilities = video.prob_predictions[i]\n","    for key, value in probabilities.items():\n","      labels.append(key)\n","      pb.append(value)\n","\n","    # Create the barplot in the second subplot\n","\n","    y_pos = np.arange(len(labels))\n","    ax2.bar(y_pos, pb, align='center', alpha=0.5)\n","    ax2.bar_label\n","    ax2.set_xticks(y_pos)\n","    ax2.set_xticklabels(labels, rotation=45)\n","    ax2.set_ylabel('Probability in %')\n","    ax2.set_title('Emotion probabilities')\n","    label = video_labels[i]\n","    ax1.set_title(f'frame_{i}.png - Labeled \"{label}\"')\n","    for j, prob in enumerate(pb):\n","        ax2.text(j, prob + 0.01, str(round(prob, 2)), ha='center', va='top')\n","    # Show the plot\n","    plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Rsq--EYOw0B"},"outputs":[],"source":["def plot_predictions_probabilities(video: LabeledVideo, prediction_type: str):\n","  if prediction_type == 'simple':\n","    data = video.prob_predictions_simple\n","  elif prediction_type == 'full':\n","    data = video.prob_predictions\n","  df = pd.DataFrame.from_records(data)\n","  ax = df.plot(figsize=(15,10))\n","  ax.set_xlabel(\"Frames\")\n","  ax.set_ylabel(\"Probability in %\")\n","  plt.title(\"Prediction Probabilities in \" + video.video_name)\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bpJDwhzGkxTr"},"outputs":[],"source":["def plot_emotions_in_video(video: LabeledVideo, prediction_type: str):\n","  \n","  if prediction_type == 'simple':\n","    pred = np.array(video.predictions_simple)\n","  elif prediction_type == 'full':\n","    pred = np.array(video.predictions)\n","  pred = np.array(video.predictions_simple)\n","  lab = np.concatenate(video.labels.values)\n","\n","  ratio_pred = {}\n","  ratio_lab = {}\n","  for elem in set(pred):\n","    ratio_pred[elem] = np.sum(pred==elem)/len(pred)*100\n","\n","  for elem in set(lab):\n","    ratio_lab[elem] = np.sum(lab==elem)/len(lab)*100\n","\n","  df = pd.DataFrame.from_records([ratio_pred, ratio_lab])\n","  df = df.fillna(0).transpose()\n","  df.rename(columns = {0:'Prediction', 1:'Label'}, inplace = True)\n","\n","  ax = df.plot.bar(rot=0, figsize=(10,5))\n","  plt.xlabel(\"Emotion\")\n","  plt.ylabel(\"Part of video in %\")\n","  plt.title(\"Aggregated emotions in \" + video.video_name)\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygP2DcmsnbTC"},"outputs":[],"source":["def create_timeline_plot(video: LabeledVideo, prediction_type: str):\n","  # Define the emotions and their corresponding colors\n","\n","  emotions = ['disgust', 'sad', 'angry', 'fear', 'neutral', 'surprise', 'happy']\n","  colors = ['green', 'blue', 'red', 'purple', 'gray', 'orange', 'yellow']\n","  # Load the list of emotions\n","  if prediction_type == 'simple':\n","    emotion_list = video.predictions_simple\n","  elif prediction_type == 'full':\n","    emotion_list = video.predictions\n","  frames_dir = video.frames_directory\n","  # Set up the figure and axes\n","  fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 6))\n","  ax1.set_xlim([0, len(emotion_list)])\n","  ax1.set_ylim([-0.5, len(emotions)-0.5])\n","  ax1.set_yticks(range(len(emotions)))\n","  ax1.set_yticklabels(emotions)\n","  ax1.set_title('Emotion Timeline')\n","\n","  ax2.axis('off')\n","  ax2.set_title('Video Frames')\n","\n","  # Create a dictionary to store the rectangles for each emotion in each frame\n","  emotion_rects = {}\n","\n","  # Define the animation function\n","  def animate(i):\n","      # Load the frame image\n","      frame_path = frames_dir+f\"frame_{i}.png\"\n","      frame = plt.imread(frame_path)\n","\n","      # Update the image in the plot\n","      im = ax2.imshow(frame, aspect='auto', extent=[i, i+1, 0, 1], zorder=0)\n","\n","      # Update the rectangles for the current emotion\n","      for j, emotion in enumerate(emotions):\n","          if emotion not in emotion_rects:\n","              emotion_rects[emotion] = []\n","          if emotion_list[i] == emotion:\n","              rect = plt.Rectangle((i, j-0.25), 1, 0.5, color=colors[j], zorder=1)\n","              ax1.add_patch(rect)\n","              emotion_rects[emotion].append(rect)\n","          else:\n","              emotion_rects[emotion].append(None)\n","\n","      return [im] + list(filter(None, emotion_rects.values()))\n","\n","  # Create the animation\n","  anim = FuncAnimation(fig, animate, frames=len(emotion_list), interval=500)\n","\n","  # Save the animation to a file\n","  save_path = base_path + f\"outputs/emotion_timeline_{video.video_name}_{prediction_type}.mp4\"\n","  Writer = animation.writers['ffmpeg']\n","  writer = Writer(fps=2, metadata=dict(artist='Me'), bitrate=1800)\n","  anim.save(save_path, writer=writer)\n","\n","  return save_path\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQUvf2K5Hjl9"},"outputs":[],"source":["def display_video(save_path):\n","  with open(save_path, \"rb\") as video_file:\n","    video_bytes = video_file.read()\n","\n","  encoded_video = b64encode(video_bytes).decode()\n","\n","  video_html = f'<video controls alt=\"test\" src=\"data:video/mp4;base64,{encoded_video}\">'\n","\n","  return HTML(video_html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_qh59xrPHw8"},"outputs":[],"source":["def threshold_smooth(emotions, threshold):\n","\n","  pairs = []\n","  count = 1\n","  last_emo = emotions[0]\n","\n","  for i in range(1,len(emotions)):\n","    if emotions[i] == last_emo:\n","      count+=1\n","    else:\n","      pair = [last_emo, count]\n","      pairs.append(pair)\n","      last_emo = emotions[i]\n","      count = 1\n","  pair = [last_emo, count]\n","  pairs.append(pair)\n","\n","  for i in range(1,len(pairs)):\n","    if pairs[i][1] < threshold:\n","      pairs[i][0] = pairs[i-1][0]\n","\n","  final = []\n","  for pair in pairs:\n","    final.extend([pair[0]]*pair[1])\n","  \n","  return final"]},{"cell_type":"markdown","metadata":{"id":"7BmR38g0_LNe"},"source":["HMM - Hidden Markov Model smoother <br>\n","We considered using it instead of the the other smoothing method. In theory it could provide better results but since we didn't have the resources (enough data) to train it on we chose more simple approach."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYyJeBJdtZ4T"},"outputs":[],"source":["# from hmmlearn import hmm\n","\n","# hmm_folder = \"/content/drive/My Drive/Colab Notebooks/Final Project/data/hmm\"\n","# len_folder = len(os.listdir(hmm_folder))\n","# sequence_emo_for_fit = []\n","\n","# for num in range(1,len_folder + 1):  \n","#   hmm_file_path = f\"/content/drive/My Drive/Colab Notebooks/Final Project/data/hmm/label_emo{num}.csv\"\n","#   df_temp = pd.read_csv(hmm_file_path)\n","#   temp_labels = df_temp['emotion'].tolist()\n","#   sequence_emo_for_fit.append(temp_labels)\n","# def hmm_smoothing(predictions, train_data):\n","#   # Define the possible states and observations\n","#   states = [\"surprise\", \"happy\", \"neutral\", \"sad\", \"angry\", \"disgust\", \"fear\"]\n","\n","#   emotion_map = {\"happy\": 0, \"sad\": 1, \"neutral\": 2, \"angry\": 3, \"surprise\": 4, \"fear\": 5, \"disgust\": 6}\n","#   numeric_sequence_for_fit = []\n","#   for lst in train_data:\n","#     numeric_labels = [emotion_map[label] for label in lst]\n","#     numeric_sequence_for_fit.append(numeric_labels)\n","\n","#   print(numeric_sequence_for_fit)\n","#   predictions_numeric = [emotion_map[label] for label in predictions]\n","#   observations = np.array(numeric_sequence_for_fit)\n","#   integer_labels = np.array(predictions_numeric)\n","\n","#   # Define the transition matrix (probabilities of moving from one state to another)\n","#   transition_matrix = np.array([\n","#       [0.6, 0.1, 0.06, 0.02,0.08,0.04,0.1],\n","#       [0.1, 0.7, 0.13, 0.01,0.02,0.02,0.02],\n","#       [0.08, 0.12, 0.48, 0.12,0.08,0.06,0.06],\n","#       [0.02, 0.02, 0.1, 0.68,0.08,0.04,0.06],\n","#       [0.02, 0.02, 0.14, 0.08,0.64,0.04,0.06],\n","#       [0.09, 0.01, 0.09, 0.04,0.06,0.64,0.07],\n","#       [0.065, 0.01, 0.08, 0.085,0.06,0.06,0.64]\n","#   ])\n","\n","#   # Define the emission matrix (probabilities of observing a certain value in each state)\n","#   emission_matrix = np.array([\n","#       [0.7,0.07,0.01,0.03,0.05,0.04,0.1],\n","#       [0.08, 0.74, 0.13, 0.01,0.02,0.01,0.01],\n","#       [0.02, 0.15, 0.63, 0.14,0.02,0.02,0.02],\n","#       [0.02, 0.02, 0.15, 0.66,0.07,0.03,0.05],\n","#       [0.09, 0.07, 0.04, 0.04,0.63,0.05,0.08],\n","#       [0.06, 0.05, 0.05, 0.04,0.08,0.64,0.08],\n","#       [0.07, 0.01, 0.03, 0.08,0.07,0.05,0.69]\n","#   ])\n","\n","#   # Define the initial state distribution (probabilities of starting in each state)\n","#   start_probabilities = np.array([0.14, 0.14, 0.16, 0.14, 0.14, 0.14, 0.14])\n","\n","#   # Create the HMM object\n","#   model = hmm.MultinomialHMM(n_components=len(states))\n","#   model.startprob_ = start_probabilities\n","#   model.transmat_ = transition_matrix\n","#   model.emissionprob_ = emission_matrix\n","\n","#   # Fit the model to the sequence of observations using the Baum-Welch algorithm\n","#   model.fit(observations)\n","\n","#   # Predict the most likely sequence of states that generated the sequence of observations\n","#   predicted_states = model.predict(predictions_numeric)\n","#   # print(\"Predicted sequence of states:\", [states[i] for i in predicted_states])\n","#   return predicted_states\n","\n","\n","# pred_test = hmm_smoothing(videos[0].predictions_simple,sequence_emo_for_fit)\n","# pred_test"]},{"cell_type":"markdown","source":["Loading the videos and labels from the directory"],"metadata":{"id":"qyCxJgqhXbVv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiwVlTAJVc85"},"outputs":[],"source":["folder = base_path + \"data/videos\"\n","videos_names = []\n","for item in os.listdir(folder):\n","    videos_names.append(item.split(\".\")[0])\n","videos_names = sorted(videos_names, key=lambda x: int(\"\".join([i for i in x if i.isdigit()])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6eBMTrz7XLvQ"},"outputs":[],"source":["videos = []\n","videos_dir = base_path + \"data/videos/\"\n","labels_dir = base_path + \"data/labels/\"\n","for v in videos_names:\n","  video = LabeledVideo(v, videos_dir+v+\".mp4\", labels_dir+v+\".csv\")\n","  video.save_video_frames(5)\n","  videos.append(video)\n"]},{"cell_type":"markdown","metadata":{"id":"6Tqh598muwPj"},"source":["Right now we are working with video of format \".mp4\" only. <br>\n","This code can help convert any other video format to mp4 format for future purposes.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HC4k8O-w-dBW"},"outputs":[],"source":["# video ='/content/drive/My Drive/Colab Notebooks/Final Project/data/videos/video4.MOV'\n","# subprocess.run(['ffmpeg',  # Convert videos to mp4 if needed (not all videos will be in mp4 format)\n","#                 '-i',\n","#                 video,\n","#                 '-qscale',\n","#                 '0',\n","#                 'video4.mp4',\n","#                 '-loglevel',\n","#                 'quiet']\n","#               )\n","# # ipd.Video(video, embed=True, width=600,height=400) // To dislpay the video"]},{"cell_type":"markdown","metadata":{"id":"S9IpymBPvLd0"},"source":["Taking a look at an example of the frames we extracted:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0fvXr4njNog"},"outputs":[],"source":["fig, axs = plt.subplots(4, 4, figsize=(20, 10))\n","axs = axs.flatten()\n","x = 0\n","img_idx = 0\n","for i in videos[0].sorted_frames:\n","  if img_idx >= 16:\n","    break\n","  if x % 2 != 0:\n","    x += 1\n","    continue\n","  img = cv2.imread(videos[0].frames_directory+i, cv2.IMREAD_COLOR)\n","  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","  axs[img_idx].imshow(img)\n","  axs[img_idx].set_title(f'Frame: {x}')\n","  axs[img_idx].axis('off')\n","  img_idx += 1\n","  x += 1\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"t670c23Y634o"},"source":["Our videos will vary in the length and resolution. <br> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdO8ojA_64IQ"},"outputs":[],"source":["# Video height and width\n","cap = cv2.VideoCapture(videos[0].video_path)\n","height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n","print(f'Height {height}, Width {width}')"]},{"cell_type":"markdown","metadata":{"id":"f0H8i-7oTq0C"},"source":["Display the image in the original size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVANGuugpGRP"},"outputs":[],"source":["img = cv2.imread(videos[0].frames_directory+\"frame_0.png\", cv2.IMREAD_COLOR)\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","plt.imshow(img)\n","plt.title(\"Original Image\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"CYEXxItKz4Te"},"source":["Simple predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwVYB4lf0Yft"},"outputs":[],"source":["simple_emotion_prob, simple_dominant_emotion = make_predictions(videos, 'retinaface', 'simple') # For the simple predictions we will use only 'retinaface' detector which supposed to be the best."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FG08x1yu1E5F"},"outputs":[],"source":["for v in videos:\n","  v.predictions_simple = simple_dominant_emotion[v.video_name]\n","  v.prob_predictions_simple = simple_emotion_prob[v.video_name]\n","  v.calculate_simple_accuracy()"]},{"cell_type":"markdown","metadata":{"id":"pRjIvC_82mEW"},"source":["Simple visualizations"]},{"cell_type":"markdown","metadata":{"id":"3-MnR5DqBRUk"},"source":["We will use specific videos for demonstrations of our plots to highlight interesting conclusions. "]},{"cell_type":"markdown","metadata":{"id":"dxmY1gKstS1x"},"source":["First we will display a confusion matrix for all videos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCL8hhcOkuF5"},"outputs":[],"source":["for v in videos: \n","  v.plot_confusion_matrix(\"simple\")"]},{"cell_type":"markdown","metadata":{"id":"e4AG9SvHtpnN"},"source":["After analyzing the results, we concluded that in general our simple predictions are not so stable. <br>\n","There are many \"soft\" missclassified emotions between 'neutral' and mainly 'angry' but also other similar emotions like sad or happy. <br>\n","Many faces can considered neutral or more decisive emotion depeneds on the context and we suspect this is the main reason for many of those mistakes. "]},{"cell_type":"markdown","metadata":{"id":"tn3ll8R9qJfP"},"source":["Lets domenstrate two \"spikey\" plots that will showcase the importance of leveraging the element of time when working with videos and not photos. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwuq9VAnCIv_"},"outputs":[],"source":[" plot_emotion_range(videos[1], \"simple\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-iAzWOFtJX9"},"outputs":[],"source":[" plot_emotion_range(videos[6], \"simple\")"]},{"cell_type":"markdown","metadata":{"id":"EnOFGMB4wX3Z"},"source":["This is an example of misclassified labels due to individual opinion of the emotion presented in the frame. <br>\n","When dealing with emotions the label is not necessarily the objective truth. One can look at the same face and interept it differently.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HAqpmaOtLJ1"},"outputs":[],"source":[" plot_emotion_range(videos[8], \"simple\")"]},{"cell_type":"markdown","metadata":{"id":"ZTIMefRI9Y_L"},"source":["In this video we see an interesting case of model uncertainty. For some frames, the probabilities are very close, and in one of them they almost match!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuntOOno9LDF"},"outputs":[],"source":["show_misclassed_frames(videos[3], \"simple\")"]},{"cell_type":"markdown","metadata":{"id":"bCiu6nes9hl-"},"source":["Here the confusion between angry and neutral is presented very well, as some frames are labeled neutral and predicted angry, and some are the exact opposite."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4ldXerz9f2S"},"outputs":[],"source":["show_misclassed_frames(videos[5], \"simple\")"]},{"cell_type":"markdown","metadata":{"id":"4Ebk8kFbJgU2"},"source":["This plot describes the probability (y axis) of every emotion for every frame (x axis). <br> The plot looks messy since the predictions of this video are not confident or smooth. It may happen due to poor model, low quality and noisy frames and the lack of consideration for time-element analysis. <br>\n","Our goal is to improve some of those aspects throughout the project for predict the emotion better. \n"," "]},{"cell_type":"markdown","metadata":{"id":"J7QWMiXG9xko"},"source":["At the beginning of this video the certainty is low, and the emotions “mix up”. At the end we see this mix up happening again. As we’ve seen before in the misclassed frames plot, this is due to low probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bh1so24I9oKJ"},"outputs":[],"source":["plot_predictions_probabilities(videos[3], \"simple\")"]},{"cell_type":"markdown","metadata":{"id":"xohM2UbyKuil"},"source":["We will demonstrate a clean and confident plot. <br>\n","Here the certainty of the model is much higher and very clear."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rbl6tsFNJXHK"},"outputs":[],"source":["plot_predictions_probabilities(videos[0], \"simple\")"]},{"cell_type":"markdown","metadata":{"id":"Xu-eiEwF-EAU"},"source":["This plot presents the predicted emotions as part of the full video, compared to the label. In this specific video there are 3 emotions that are predicted  but do not appear at all in the labels, as well as a very poor recognition of the emotion that is dominant throughout the video."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEP7oPy49-p2"},"outputs":[],"source":["plot_emotions_in_video(videos[16], \"simple\")"]},{"cell_type":"markdown","metadata":{"id":"JW8ijY4zMR_8"},"source":["Due to memory and run time considerations with Colab Notebook we will only display the following animated plot. <br> The animations were preproduced using the function `create_timeline_plot`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQYIbGi7Xh0w"},"outputs":[],"source":["# demo_timeline_path = create_timeline_plot(demo_video, \"simple\")"]},{"cell_type":"markdown","metadata":{"id":"ZAWjRUl7YT8A"},"source":["The animated plot is made to present the emotions detected as a timeline of some sort, while presenting the frames.\n","The plot shows rectangles that are very small, another way to see the “spikes” in emotions discussed before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXKkT3D8NTTa"},"outputs":[],"source":["demo_timeline_path = base_path + 'outputs/emotion_timeline_video2_simple.mp4' # Hard-coded for reasones mentioned above \n","display_video(demo_timeline_path)"]},{"cell_type":"markdown","metadata":{"id":"oWy1u_Nc3z52"},"source":["#Preprocess images before re-analyzing for better predictions "]},{"cell_type":"markdown","metadata":{"id":"5KvNc7ZiB3h9"},"source":["We want to separate the face emotion analyze to two main parts: <br>\n","1. Extract the face from the image <br>\n","We do so because that way we can handle blurred or damaged images with a lot of noise. If we would use the analyze function without separating we would not be abke to do so. <br>\n","This will make the running time longer but will increase the quality of our predictions. Since we are working on a limited size of videos and expecting a user to use one video at a time we are willing to take that cost. <br>\n","If we found a blurred image using the variance of laplacian method, we then apply wiener filter for deblurring and denoising the frame. We also resizing each image to a constant and much smaller size which will reduce running time later on when analyzing the image to recognize the emotion. <br>\n","2. Analyze and predict the emotion. Will be done later on."]},{"cell_type":"markdown","metadata":{"id":"aBHfGnea3-dI"},"source":["##Blurred images"]},{"cell_type":"markdown","metadata":{"id":"xTbu213k38A6"},"source":["Images may be blurred due to resizing, camera shake, noise or other issues. <br>\n","We will try to detect those images with the help of Variance of Laplacian (VoL) method. We chose it over Fast Fourier Transform, blind image de-convolution, Edge sharpness analysis and others that we read about. It worked well for us and the running time was good compare to other methods as well. <br>\n","If we will find that the image is blurred, we will apply wiener method to deblurr the image. Once again we tried several methods (even just sharpening filter rather then deblurring algorithm) and chose wiener for it worked well and had suitable running time for our purposes and means.<br>"]},{"cell_type":"markdown","metadata":{"id":"XknlwPdaX9S8"},"source":["##Face detection and alignment"]},{"cell_type":"markdown","metadata":{"id":"FDDMyTEM4tWp"},"source":["One of the cool features of the library is that for detecting faces the image runs through few modules: 2D alignment, 3D alignment and Frontalization. <br>\n","It uses fiducial points when making the alignment and trying to \"frontalize\" the face towards the camera after those alignments."]},{"cell_type":"markdown","metadata":{"id":"drIviKWgNuq9"},"source":["There are several backend detectors for the face recognition. <br> After reasearch, the best in terms of accuracy is retinaface and mtcnn. If the run time is more important we should go with ssd. <br> If ssd will perform well, we might even go with it.<br>\n","We tested a few options and decided to go with retinaface since it provided the best results. Its run time is on the higher side but acceptable for our purposes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoXw0GuXPOPU"},"outputs":[],"source":["c_blur = 0\n","threshold = 0.00026 # Literature support the value 30 if we want to take only the really blurred images. Lower threshold support blur detection for really blurred images but in our case,\n","# because we handle face images, we needed to adjust the threshold since it is not corresponding well with the variance calculated on this images.Divide it by 1000 and stay around this value\n","for v in videos:\n","  deblurr_flag = False\n","  temp_boxes = []\n","  for frame in v.sorted_frames:\n","    face = DeepFace.extract_faces(img_path = v.frames_directory + frame,  # The face that will return will be in RGB according to documentation\n","        target_size = (224, 224), \n","        detector_backend = 'retinaface',\n","        enforce_detection = 'False')\n","    if len(face) == 0:\n","      print(\"No faces detected in the image.\")\n","      continue\n","    img = face[0]['face'] # Taking the face that extracted \n","    box = face[0]['facial_area'] # Taking the box of coordinates\n","    temp_boxes.append(box)\n","    variance = variance_of_laplacian(img)\n","    if variance < threshold: #Image is blurred\n","      if deblurr_flag == False:\n","        print(\"Variance is:\", variance)\n","        plt.imshow(img)\n","        plt.title(\"Before Deblurring Image\")\n","        plt.show()\n","      f, Pxx = welch(img, fs=1, nperseg=256) # Trying to estimate the noise to use in Wiener algorithm. sampling frequency is 1 and default length of each segment\n","      estimated_noise = np.mean(Pxx)\n","      for c in range(3): # Iterating with wiener filter for each channel (color) separately since wiener suppose to work with 1 dimension\n","          img[:, :, c] = wiener(im= img[:, :, c], mysize=(5, 5), noise= estimated_noise)\n","      c_blur = c_blur + 1\n","      if deblurr_flag == False:\n","        plt.imshow(img)\n","        plt.title(\"After Deblurring Image\")\n","        plt.show()\n","        deblurr_flag = True\n","      img = img_as_ubyte(img)\n","    imsave(v.frames_wip_directory + frame, img)\n","  v.boxes = temp_boxes\n","print(\"Amount of images that was repaired and deblurred from all the videos: \", c_blur)"]},{"cell_type":"markdown","metadata":{"id":"BpzsPBHfxnne"},"source":["It was a difficult task to deblurr images from an unknown distribution, even harder with the resources we had. <br>. To estimate it properly we tried different algorithms that caused us a lot of run time and the results was not much better. <br>\n","Some images seems to be improved but some does not. Overall we saw an improvement and we chose this method of estimating noise with welch algorithm and deblurr with wiener method. "]},{"cell_type":"markdown","metadata":{"id":"o2H9VCxJyun3"},"source":["We estimated an image as blurred with `Variance of Laplacian` method. <br>\n","Basically it takes the Laplacian of the image which highlights edges and details and then according to their variance we will decide which image is blurred and which is not. High variance indicate sharper images and therefore not blurred. <br>\n","It is a well known method in computer vision for this task and introduced in the early 90' and been used ever since."]},{"cell_type":"markdown","metadata":{"id":"e5MA-RKt0Lls"},"source":["For estimating noise we use `welch` method that using Fast Fourier transform. It is particularly useful when the noise varies in intensity and frequency like what can happened with different images with different qualities, angles, frames and positions. <br>\n","These value will help us as a parameter for the deblurring algorithms that needs to know the noise in the image.  "]},{"cell_type":"markdown","metadata":{"id":"PHbcL3c-2oyZ"},"source":["`Wiener` algorithm is the one that chosen to help us deblur an image. <br>\n","It is a known and proved method and relatively computationally efficient unlike other very complex algorithms that considered to be used. <br>\n","It is a linear statistical approach that estimates the original, unblurred image\n","and uses information about the noise and blurring present in the image to estimate the optimal filter that would minimize the mean squared error between the original image and the degraded image. "]},{"cell_type":"markdown","metadata":{"id":"XGIFobZa4KQz"},"source":["Our choices of those algorithms derived from testing, experiencing, research, run time considerations and popularity in the relevant field."]},{"cell_type":"markdown","metadata":{"id":"dtyB4QynIafO"},"source":["# Predict after preprocess adjustments"]},{"cell_type":"markdown","metadata":{"id":"v6m9MnnOSgr_"},"source":["Another improvement that was considered is to average between two detectors predictions. <br> Since we try it on a small dataset we could not tell if it really better this way so for now on we will choose the way that gives us the better results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aBHHZyE8QS7-"},"outputs":[],"source":["mtcnn_emotion_prob_wip, mtcnn_dom_emotion_wip = make_predictions(videos, 'mtcnn', \"full\")\n","retinaface_emotion_prob_wip, retinaface_dom_emotion_wip = make_predictions(videos, 'retinaface', \"full\")"]},{"cell_type":"markdown","metadata":{"id":"opZjSDq0-d_s"},"source":["One possible option to make the predictions more accurate and stable is to average between two strong detectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Icp3_J9YCuE"},"outputs":[],"source":["avg_prob = {}\n","for key in retinaface_emotion_prob_wip.keys():\n","    sub_dicts = zip(mtcnn_emotion_prob_wip[key], retinaface_emotion_prob_wip[key])\n","    avg_prob[key] = [{subkey: (sub_dict[subkey] + sub_dict2[subkey]) / 2 \n","                      for subkey in sub_dict.keys()} for sub_dict, sub_dict2 in sub_dicts]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SN2WeMogjw1C"},"outputs":[],"source":["avg_dom_emotion = {}\n","for key in avg_prob.keys():\n","  temp = []\n","  for d in avg_prob[key]:\n","     temp.append(max(d, key=d.get))\n","  avg_dom_emotion[key] = temp"]},{"cell_type":"markdown","metadata":{"id":"ku6wOuadaggA"},"source":["Handling glitches in predictions. <br>\n","We will work to improve and avoid glitches and missclassified predictions with the help of the time element in these predictions. <br>\n","We will choose between moving average\\moving common mode(emotion) or Hidden Markov Model (HMM). <br>\n","We will work on the dominant emotion and not the probabilites becauese we felt that we have there more knowledge and feel safe to work on those type of predictions."]},{"cell_type":"markdown","metadata":{"id":"x9nDrVtlT6iE"},"source":["HMM was too complex and required a large amount of data that we didn't have. <br>\n","Therefore we will smooth predictions using `threshold_smooth` method.\n"]},{"cell_type":"markdown","metadata":{"id":"pT657ftPVotN"},"source":["`threshold_smooth ` works well under the assumption that emotion suppose to last for at least `x` time\\frames. <br> \n","If it happens that some emotion appears in an unlikely amount of frames we will replace it with the last verified emotion. <br>\n","Thats how we will smooth the predictions and use the time-element of our video. <br> \n","Obviously this method is not bullet-proof and we thought of several cases that won't be dealt perfectly but it is still good enough and improve our results.\n","We chose the threshold according to tests we run and consider the low fps featured in our work flow. Different fps will result in different minumum threshold. The higher the fps is the higher the threshold suppose to be.  <br> If the project scales we will add more complex smoothing methods for the predictions and the probabilites of them.  "]},{"cell_type":"markdown","metadata":{"id":"CMW6Z5cNnMr-"},"source":["We want to check if our work on the images before analyzing them was helpful or the smooth function is the reason for the improvement. <br>\n","We will caclulate few options, using the threshold algorithm. <br>\n","Comparison between simple predictions, full predictions and averaging between detectors will help us decide which option is our optimal choice for our limited dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RmqXBATlyoG"},"outputs":[],"source":["simple_smooth_dict = {}\n","retinaface_full_dict = {}\n","mtcnn_full_dict = {}\n","avg_predictions = {}\n","min_threshold = 3\n","\n","for v in videos:\n","  temp_simple = v.predictions_simple\n","  temp_simple = threshold_smooth(temp_simple, min_threshold)\n","  simple_smooth_dict[v.video_name] = temp_simple\n","  temp_retinaface = retinaface_dom_emotion_wip[v.video_name]\n","  temp_retinaface = threshold_smooth(temp_retinaface, min_threshold)\n","  retinaface_full_dict[v.video_name] = temp_retinaface\n","  temp_mtcnn = mtcnn_dom_emotion_wip[v.video_name]\n","  temp_mtcnn = threshold_smooth(temp_mtcnn, min_threshold)\n","  mtcnn_full_dict[v.video_name] = temp_mtcnn \n","  temp_avg = threshold_smooth(avg_dom_emotion[v.video_name], min_threshold)\n","  avg_predictions[v.video_name] = temp_avg\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvqNLAxUapwK"},"outputs":[],"source":["avg_full = {}\n","simple = {}\n","simple_with_smooth = {}\n","retinaface_full = {}\n","mtcnn_full = {}\n","\n","\n","for v in videos:\n","    simple[v.video_name] = v.accuracy_simple\n","    simple_with_smooth[v.video_name] = accuracy_score(v.labels, simple_smooth_dict[v.video_name])\n","    avg_full[v.video_name] = accuracy_score(v.labels, avg_dom_emotion[v.video_name])\n","    retinaface_full[v.video_name] = accuracy_score(v.labels, retinaface_full_dict[v.video_name])\n","    mtcnn_full[v.video_name] = accuracy_score(v.labels, mtcnn_full_dict[v.video_name])\n","\n","df = pd.DataFrame.from_records([simple, simple_with_smooth, avg_full, retinaface_full, mtcnn_full]).transpose()\n","df.rename(columns= {0: \"Simple\", 1: \"Simple_with_Smooth\",2: \"Average Full\", 3: \"Retinaface Full\", 4: \"MTCNN Full\"}, inplace= True)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3ivA-NK_I5V"},"outputs":[],"source":["df.describe().iloc[1:3,:]"]},{"cell_type":"markdown","metadata":{"id":"qaoAuEVkCh7-"},"source":["Even though we have a really small dataset, we will choose our option based on those results with a consideration that for a scalable project it may be a little different and will try to avoid overfit. <br>\n"]},{"cell_type":"markdown","metadata":{"id":"vNRQ_s_cC1ji"},"source":["First we can see that the smooth algorithm improved our results. It is not that clear because if a streak of predictions is wrong in the first place as we seen before, the smooth algorithm can't help in that case. <br>\n","Simple predictions are quite good even without all of the manipulations that we have done. For some videos the simple predictions accuracy was actually better. <br>\n","But still, overall we can notice some improvement in the average accuracy and a decline in the standard deviation of the results when using MTCNN detector. <br>  Again, we keep in mind that for a bigger dataset those results may differ. Therefore we will choose the more stable and strong option. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gD9rmfPeBM7N"},"outputs":[],"source":["for v in videos:\n","  v.predictions = mtcnn_full_dict[v.video_name]\n","  v.prob_predictions = mtcnn_emotion_prob_wip[v.video_name] # After changing some of the predictions the probabilities won't correlate perfectly with the dominant emotion predicted but we will not use it  \n","  v.calculate_accuracy()"]},{"cell_type":"markdown","metadata":{"id":"GJIJemN6VuWw"},"source":["Updated Visualizations for full predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtpSIotWVyxN"},"outputs":[],"source":["for v in videos:\n","  v.plot_confusion_matrix(\"full\")"]},{"cell_type":"markdown","metadata":{"id":"v2WDLfBnO4ou"},"source":["The confusion matrices strongly indicate a reocurring mistake between neutral and other emotions.<br> Generally speaking, the model predicts neutral for many other emotions. We suspect this happens becuase of two main reasons:\n","1. The context of the frames that are presented to the model as singular units but are a part of a full video with a specific sentiment.  \n","2. There are more emotions that can be displayed than the ones we have in the model. Therefore, we think many of them are being predicted as neutral whereas their label is somewhere in between. <br>\n","For example, we've noticed faces that look a bit serious or resentful, some will be predicted as angry and some as neutral.  "]},{"cell_type":"markdown","metadata":{"id":"ni6AkEPOUIXm"},"source":["We can infer another interesting conclusion. It looks like our model have a real difficulty predicting the emotion \"disgust\". <br> It may happen because it is a rare emotion and pergaps not be trained on it a lot to reach a sufficient understanding of it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_OyUksLVyxO"},"outputs":[],"source":["for v in videos:\n","  plot_emotion_range(v, \"full\")"]},{"cell_type":"markdown","metadata":{"id":"xfiAIeY4SFiN"},"source":["First of all, the emotion range plots clearly showcase the smoothing algorithm and its positive effect. No more \"spikes\" of different emotions and more stable predictions. <br>\n","Here is another way to look at the mistakes between neutral and some other emotions, but when it predict a specific emotion, it gives more solid emotions streak and shifts between emotions very well.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FjcSpL1XKlM"},"outputs":[],"source":["show_misclassed_frames(videos[2], \"full\")"]},{"cell_type":"markdown","metadata":{"id":"PnYFeHOYXeeI"},"source":["This is an example of a face that not necessarily in our scope of emotions as we talked about before."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZXx0b2PrXg9X"},"outputs":[],"source":["show_misclassed_frames(videos[5], \"full\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mamXMgNXM9v"},"outputs":[],"source":["show_misclassed_frames(videos[10], \"full\")"]},{"cell_type":"markdown","metadata":{"id":"ZtxPPnUzVEIh"},"source":["First, we will mention that the predictions have gone through smoothing operation whereas the probabilities didn't. This means that for some cases the emotion with the highest probability is not the one that has been predicted. <br>\n","A good example of it can be found in video 6 at frames 22-27 where frame 24 is highly confident about the happy emotion."]},{"cell_type":"markdown","metadata":{"id":"WPPNkilBVpIZ"},"source":["We see in videos [5, 10] examples of frames that our model was not confident about its predictions. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRHZgr0gVyxP"},"outputs":[],"source":["plot_predictions_probabilities(videos[2], \"full\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKfneTe0aKsQ"},"outputs":[],"source":["plot_predictions_probabilities(videos[4], \"full\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRrnMi8JaK8d"},"outputs":[],"source":["plot_predictions_probabilities(videos[7], \"full\")"]},{"cell_type":"markdown","metadata":{"id":"w_WlofJoaby3"},"source":["In these examples, we see that when the model is not confident it looks like a total mess. But when it is more confident about the results there is one emotion with significantly higher probability."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCp4wnwgVyxQ"},"outputs":[],"source":["plot_emotions_in_video(videos[2], \"full\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBrY2IIccEtR"},"outputs":[],"source":["plot_emotions_in_video(videos[5], \"full\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLdgkpUTcFzz"},"outputs":[],"source":["plot_emotions_in_video(videos[12], \"full\")"]},{"cell_type":"markdown","metadata":{"id":"e4qEtDYCcKA4"},"source":["Not much to say, when the predictions are correct and at the same time (which is not necessarily showen in this plot) those bars match, and when they not the predictions and labels fill different bars (emotions). "]},{"cell_type":"markdown","metadata":{"id":"tzD_gIREXO3Y"},"source":["Once again, it takes too much RAM so we run it locally and will display the outputs in the next cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ITDFUByVXIZ-"},"outputs":[],"source":["# for v in videos:\n","#   create_timeline_plot(v, \"full\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_hUDv5MVyxQ"},"outputs":[],"source":["output_dir = os.listdir(base_path + \"outputs\")\n","path_videos_to_display = []\n","videos_to_display = [3,4,6,14] # Choose videos to display\n","for i in range(1,len(output_dir) + 1):\n","  if i in videos_to_display:\n","    path = base_path + f\"outputs/emotion_timeline_video{i}_full.mp4\"\n","    path_videos_to_display.append(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ru2QvW9AptVD"},"outputs":[],"source":["display_video(path_videos_to_display[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMnzCHe2p5Ct"},"outputs":[],"source":["display_video(path_videos_to_display[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wkJxdi6p5Nw"},"outputs":[],"source":["display_video(path_videos_to_display[2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZuLr9-7p500"},"outputs":[],"source":["display_video(path_videos_to_display[3])"]},{"cell_type":"markdown","metadata":{"id":"r1u68tQ6ltwS"},"source":["Exporting a new video of the original frames that were extracted with their face box + predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rqy8GT6-hUPL"},"outputs":[],"source":["def add_annotations(img, box, emotion, font_scale, thickness):\n","  x = box['x']\n","  y = box['y']\n","  w = box['w']\n","  h = box['h']\n","  annotated_img = cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), thickness)\n","  annotated_img = cv2.putText(img, emotion, (x, y-10), cv2.FONT_HERSHEY_TRIPLEX, font_scale, (0, 255, 0), thickness)\n","  return annotated_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G58eGXx5jjUT"},"outputs":[],"source":["pred_video_path = base_path + \"prediction_video/\" \n","if not os.path.exists(pred_video_path):\n","  os.mkdir(pred_video_path)\n","for v in videos:\n","    \n","  # Define the folder path and get the list of frames\n","  folder_path = v.frames_directory\n","  frame_list = v.sorted_frames\n","\n","  img = cv2.imread(folder_path + frame_list[0])\n","  height = img.shape[0]\n","  width = img.shape[1]\n","  VIDEO_CODEC = \"mp4v\"\n","  \n","  THICKNESS_SCALE = 3e-3\n","  FONT_SCALE = 4e-3\n","  font_scale = min(width, height) * FONT_SCALE\n","  thickness = math.ceil(min(width, height) * THICKNESS_SCALE)\n","\n","  bbox_list = v.boxes\n","\n","  # Define the video writer\n","  out = cv2.VideoWriter(pred_video_path + f'{v.video_name}.mp4',  cv2.VideoWriter_fourcc(*VIDEO_CODEC), 5, (width, height))\n","\n","  # Loop through the frames and apply the bounding boxes\n","  for i, frame_name in enumerate(frame_list):\n","      frame_path = os.path.join(folder_path, frame_name)\n","      frame = cv2.imread(frame_path)\n","\n","      # Get the bounding box for the current frame\n","      bbox = bbox_list[i]\n","\n","      # Draw the bounding box on the frame\n","      frame = add_annotations(frame, bbox, v.predictions[i], font_scale, thickness)\n","\n","      # Write the frame to the video writer\n","      out.write(frame)\n","      v.predicted_video_path = pred_video_path + f'{v.video_name}.mp4'\n","  # Release the video writer and close all windows\n","  out.release()\n","  cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"g1g4kzFUdR1B"},"source":["The videos with the predictions and the bounding box of the detections will be exported to \"prediction_video\" directory. <br> \n","We recommend you to watch video 2, video 4 and video 16 🙂"]},{"cell_type":"markdown","metadata":{"id":"P7Sspl9AoQcQ"},"source":["## Spider plot\n","Unfortunately we didn't have time to finish this plot as we wanted to, for future work we decided to keep it hidden and to come back to it later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wR0a2zceKM-"},"outputs":[],"source":["# # create a video of spyder anlysis graph\n","# import matplotlib.pyplot as plt\n","# from matplotlib.animation import FuncAnimation\n","# import numpy as np\n","\n","# # Define the labels for the spider chart\n","# labels = ['neutral','sad','angry','disgust','happy','surprise', 'fear']\n","\n","# # Create a figure and axis with polar projection\n","# fig, ax = plt.subplots(subplot_kw=dict(projection=\"polar\"))\n","\n","# num_angles = len(labels)\n","# angles = np.linspace(0, 2*np.pi, num_angles+1)[:-1]\n","\n","# # Set the angle values as the x tick positions\n","# ax.set_xticks(angles)\n","\n","# # Set the angle labels\n","# ax.set_xticklabels(labels)\n","\n","# # Set the radial limits and the radial grid lines\n","# ax.set_rlim(0, 1)\n","# ax.grid(True)\n","\n","# # Define the patches for each label\n","# patches = []\n","# for i in range(num_angles):\n","#     patch = plt.Rectangle((0, 0), 0, 0, alpha=0.7)\n","#     patches.append(patch)\n","#     ax.add_patch(patch)\n","\n","# point = ax.scatter(angles[0], 0, marker=\"o\", color=\"red\", s=50)\n","\n","# # Define a function to update the spider chart for each frame\n","# def update(frame):\n","#     # Generate new data for each second\n","#     data = np.random.dirichlet(np.ones(num_angles))\n","    \n","#     # Set the color and position of each patch\n","#     max_value_index = np.argmax(data)\n","#     for i in range(num_angles - 1):\n","#         patch = patches[i]\n","#         value = data[i]\n","#         patch.set_width(value)# * np.cos(angles[i+1]-angles[i])/2)\n","#         patch.set_height(value)# * np.sin(angles[i+1]-angles[i])/2)\n","#         patch.set_xy((angles[i], 0))\n","#         patch.set_facecolor(plt.cm.tab20(i/num_angles))\n","#         patch.set_transform(ax.transData)\n","    \n","    \n","#     # Get the value and angle for the highest value\n","#     value = data[max_value_index]\n","#     angle = angles[max_value_index]\n","    \n","#     # Update the position of the point\n","#     point.set_offsets((angle, value))\n","    \n","#     # Return the point as a tuple\n","#     return patches\n","\n","# # Create the animation\n","# animation = FuncAnimation(fig, update, frames=11, interval=1000, blit=True)\n","\n","# # Save the animation as a video file\n","# animation.save(base_path + \"spider.mp4\", dpi=100)\n","\n","# # Show the final plot (optional)\n","# plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJyuy1v3EOB_"},"outputs":[],"source":["# import cv2\n","# import os\n","# # Define the video capture objects for the two videos\n","# cap1 = cv2.VideoCapture(base_path + 'data/videos/video3.mp4')\n","# cap2 = cv2.VideoCapture(base_path + 'spider.mp4')\n","\n","# # Get the frame rate of the first video\n","# fps1 = cap1.get(cv2.CAP_PROP_FPS)\n","\n","# # Get the frame rate of the second video\n","# fps2 = cap2.get(cv2.CAP_PROP_FPS)\n","\n","# # Get the width and height of the videos\n","# width1 = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n","# height1 = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","# width2 = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n","# height2 = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# # Resize the second video\n","# scale_factor = height1 / height2\n","# new_width2 = int(width2 * scale_factor - 200)\n","# new_height2 = height1\n","# while new_width2 % 4 != 0:  # Make sure the width is divisible by 4 (required by VideoWriter)\n","#     new_width2 += 1\n","# frame_size2 = (new_width2, new_height2)\n","# cap2_resized = cv2.VideoWriter('/content/drive/My Drive/Colab Notebooks/Final Project/spider_resized.mp4', fourcc, fps1, frame_size2)\n","# while True:\n","#     ret, frame = cap2.read()\n","#     if not ret:\n","#         break\n","#     frame_resized = cv2.resize(frame, frame_size2)\n","#     cap2_resized.write(frame_resized)\n","# cap2.release()\n","# cap2_resized.release()\n","# cap2_resized = cv2.VideoCapture('/content/drive/My Drive/Colab Notebooks/Final Project/spider_resized.mp4')\n","\n","# # Define the margin between the two videos\n","# margin = 150\n","\n","# # Create a VideoWriter object to write the output video\n","# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","# out = cv2.VideoWriter('/content/drive/My Drive/Colab Notebooks/Final Project/output.mp4', fourcc, fps2, (width1+new_width2+margin, height1))\n","\n","# while True:\n","#     # Read a frame from the first video\n","#     ret1, frame1 = cap1.read()\n","    \n","#     # Read a frame from the second video\n","#     ret2, frame2 = cap2_resized.read()\n","    \n","#     # Check if both videos have reached the end\n","#     if not ret1 or not ret2:\n","#         break\n","    \n","#     # Resize the frames to fit in the output video\n","#     frame1 = cv2.resize(frame1, (width1, height1))\n","    \n","#     # Concatenate the frames horizontally with a margin\n","#     frames = cv2.hconcat([frame1, np.zeros((height1, margin, 3), np.uint8), frame2])\n","    \n","#     # Write the concatenated frames to the output video\n","#     out.write(frames)\n","\n","# # Release the resources\n","# cap1.release()\n","# cap2_resized.release()\n","# out.release()\n","\n","# # Delete the resized second video file\n","# os.remove('/content/drive/My Drive/Colab Notebooks/Final Project/spider_resized.mp4')\n"]}],"metadata":{"colab":{"collapsed_sections":["P7Sspl9AoQcQ"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
